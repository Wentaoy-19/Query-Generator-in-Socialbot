{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from eval_F1 import f1_score,metric_max_over_ground_truths\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import numpy as np \n",
    "import os \n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm \n",
    "rouge = Rouge()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "def prepare_prompt_query(context,entity):\n",
    "    # with cosmo\n",
    "    prompt = f\"You are given a short dialog between a user and a bot for a discussion on {entity}. Convert the bot response to a question to use for internet search to get relevant knowledge for continuing the response.\\n\\n\"\n",
    "    # # # without cosmo\n",
    "    # prompt = f\"You are given a short dialog between a user and a bot for a discussion on {entity}. For the next bot response, generate a search query to use for the internet\\n\\n\"\n",
    "    prompt += context\n",
    "    prompt += \"\\n\\nquestion:\"\n",
    "    \n",
    "    # prompt = context + \"\\n\\nquestion:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def prepare_prompt_entity(context:str):\n",
    "    final_prompt = \"You are given a dialog between bot and user.  You need to identify the current Topic being discussed.\\n\\n\"+ context + \"\\n\\nTopic: \"\n",
    "    return final_prompt\n",
    "\n",
    "def prepare_prompt_entity_train(context):\n",
    "    turns = len(context)\n",
    "    dialog_text =\"\"\n",
    "    for i in range(turns):\n",
    "        dialog_text += \"\\nUser1: \" + context[i][\"user1\"] + \"\\nUser2: \"+context[i][\"user2\"]\n",
    "    final_prompt = \"You are given a dialog between user1 and user2.  You need to identify the current Topic being discussed.\\n\\n\"+ dialog_text + \"\\n\\nTopic: \"\n",
    "    return final_prompt\n",
    "\n",
    "def query_inference(context: str, model,entity:str):\n",
    "    input_text = prepare_prompt_query(context,entity)\n",
    "    # print(input_text)\n",
    "    input_ids = tokenizer(input_text,return_tensors = 'pt').input_ids.cuda()\n",
    "    outputs = model.generate(input_ids, max_new_tokens=40, num_beams=4)\n",
    "    query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(query)\n",
    "    return query\n",
    "\n",
    "def entity_inference(context,model):\n",
    "    input_text = prepare_prompt_entity_train(context)\n",
    "    input_ids = tokenizer(input_text,return_tensors = 'pt').input_ids\n",
    "    # input_ids = tokenizer(input_text,return_tensors = 'pt').input_ids.cuda()\n",
    "\n",
    "    outputs = model.generate(input_ids, max_new_tokens=15, num_beams=4)\n",
    "\n",
    "    entity = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return entity \n",
    "    \n",
    "\n",
    "def evaluate_query(idx:int,dataset,model):\n",
    "    context = dataset[idx]['context']\n",
    "    entity = dataset[idx]['entity']\n",
    "    gt_query = dataset[idx]['query_gen']\n",
    "    # inference\n",
    "    query = query_inference(context,model,entity)\n",
    "    # print(query)\n",
    "    # print(gt_query)\n",
    "    return rouge.get_scores(query, gt_query)[0]['rouge-l']['f']\n",
    "\n",
    "def evaluate_entity(idx:int, dataset,model):\n",
    "    entity = entity_inference(dataset[idx]['context'],model)\n",
    "    gt_entity = dataset[idx]['predict']\n",
    "    score = metric_max_over_ground_truths(f1_score, entity, [gt_entity])\n",
    "    return score\n",
    "\n",
    "def evaluate_entity_main(dataset_path,model_path):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).cuda()\n",
    "    fp = open(dataset_path)\n",
    "    dataset = json.load(fp)\n",
    "    length = 1000\n",
    "    score = 0\n",
    "    for i in tqdm(range(length)):\n",
    "        _score = evaluate_entity(i,dataset,model)\n",
    "        # print(_score)\n",
    "        score += _score\n",
    "    score/= length\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "def evaluate_query_main(dataset_path,model_path):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).cuda()\n",
    "    fp = open(dataset_path)\n",
    "    dataset = json.load(fp)\n",
    "    length = 1\n",
    "    score = 0\n",
    "    for i in tqdm(range(length)):\n",
    "        _score = evaluate_query(i,dataset,model)\n",
    "        # print(_score)\n",
    "        score += _score\n",
    "    score/= length\n",
    "    print(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [10:12<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2941630815862882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────────────────────────────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\"> /tmp/ipykernel_3044/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3179338953.py</span><span style=\"font-weight: bold\">:</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold\">24</span>                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">▲</span>                                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SyntaxError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'return'</span> outside function\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m /tmp/ipykernel_3044/\u001b[0m\u001b[1;33m3179338953.py\u001b[0m\u001b[1m:\u001b[0m\u001b[1;94m24\u001b[0m                                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;91m▲\u001b[0m                                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSyntaxError: \u001b[0m\u001b[32m'return'\u001b[0m outside function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _evaluate_query(idx:int,dataset1,dataset2,model):\n",
    "    context = dataset1[idx]['context']\n",
    "    entity = dataset1[idx]['entity']\n",
    "    gt_query = dataset2[idx]['query_gen']\n",
    "    # inference\n",
    "    query = query_inference(context,model,entity)\n",
    "    # print(query)\n",
    "    # print(gt_query)\n",
    "    return rouge.get_scores(query, gt_query)[0]['rouge-l']['f']\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"../query_cosmo_20k/\")\n",
    "fp1 = open(\"../dataset/train_query.json\")\n",
    "fp2 = open(\"../dataset/train_query_nocosmo.json\")\n",
    "dataset1 = json.load(fp1)\n",
    "dataset2 = json.load(fp2)\n",
    "length = 1000\n",
    "score = 0\n",
    "for i in tqdm(range(length)):\n",
    "    _score = _evaluate_query(i,dataset1,dataset2,model)\n",
    "    # print(_score)\n",
    "    score += _score\n",
    "score/= length\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2941630815862882"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
